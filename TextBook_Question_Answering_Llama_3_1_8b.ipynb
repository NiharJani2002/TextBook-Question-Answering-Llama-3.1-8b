{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DownLoad Libraries #"
      ],
      "metadata": {
        "id": "luCu3W2uBXRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch\n",
        "! pip install --upgrade transformers\n",
        "! pip install accelerate huggingface_hub\n",
        "\n",
        "! pip install pymupdf\n",
        "! python3 install spacy\n",
        "! python3 -m spacy download en_core_web_trf\n",
        "! pip install transformers\n",
        "! pip install torch\n",
        "! pip install rank_bm25\n",
        "\n",
        "! python3 -m nltk.downloader wordnet\n",
        "! pip install nltk\n",
        "! pip install faiss-cpu\n",
        "\n",
        "! pip install --upgrade pip setuptools wheel\n",
        "! pip install bertopic --no-cache-dir\n",
        "! pip uninstall hdbscan -y\n",
        "! pip install hdbscan --no-cache-dir --no-binary :all: --no-build-isolation\n"
      ],
      "metadata": {
        "id": "eI4om-L8Bhgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Login #"
      ],
      "metadata": {
        "id": "EmazZC_oDawd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Replace 'your_token_here' with your actual Hugging Face token\n",
        "token = 'your_token_here'\n",
        "login(token)"
      ],
      "metadata": {
        "id": "iLwbiqxfDdsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Import Libraries #"
      ],
      "metadata": {
        "id": "LDfdf_m4BZSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import spacy\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import transformers"
      ],
      "metadata": {
        "id": "x0rf9jROCG2c"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "Styo5M8FCUTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Task 1: Text Extraction from PDF #"
      ],
      "metadata": {
        "id": "VwyKZqZ9CXMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Text Extraction from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "    pdf_document.close()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "SYXKJ_NeCWPv"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Task 2: Hierarchical Tree-based Indexing #"
      ],
      "metadata": {
        "id": "hrzQOqAbCZ19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Hierarchical Tree-based Indexing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "RirW6ENWCe_Q"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text):\n",
        "    return re.split(r'\\n{2,}', text)"
      ],
      "metadata": {
        "id": "Q1oUaFqICjJa"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_hierarchical_index(text):\n",
        "    nlp = spacy.load('en_core_web_trf')\n",
        "    classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
        "    candidate_labels = [\"Chapter\", \"Section\", \"Subsection\"]\n",
        "    tree = {\"Root\": {}}\n",
        "    current_chapter = None\n",
        "    current_section = None\n",
        "    text = preprocess_text(text)\n",
        "    paragraphs = split_text(text)\n",
        "\n",
        "    for para in paragraphs:\n",
        "        if not para.strip():\n",
        "            continue\n",
        "        classification = classifier(para, candidate_labels)\n",
        "        label = classification['labels'][0]\n",
        "        if label == \"Chapter\":\n",
        "            current_chapter = para\n",
        "            tree[\"Root\"][current_chapter] = {}\n",
        "            current_section = None\n",
        "        elif label == \"Section\":\n",
        "            if current_chapter:\n",
        "                current_section = para\n",
        "                tree[\"Root\"][current_chapter][current_section] = []\n",
        "        else:\n",
        "            if current_section:\n",
        "                tree[\"Root\"][current_chapter][current_section].append(para)\n",
        "\n",
        "    flat_index = []\n",
        "    def flatten_tree(node, path=[]):\n",
        "        for key, value in node.items():\n",
        "            if isinstance(value, dict):\n",
        "                flatten_tree(value, path + [key])\n",
        "            else:\n",
        "                flat_index.append({\"path\": path + [key], \"content\": value})\n",
        "\n",
        "    flatten_tree(tree)\n",
        "    return flat_index"
      ],
      "metadata": {
        "id": "I-OCcsAqCmaY"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Retrieval Techniques #"
      ],
      "metadata": {
        "id": "tssAWjEpCotx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('facebook/dpr-question_encoder-multiset-base')\n",
        "model = AutoModel.from_pretrained('facebook/dpr-question_encoder-multiset-base')"
      ],
      "metadata": {
        "id": "E5wSFlMQCs84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_query(query):\n",
        "    inputs = tokenizer(query, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs).pooler_output\n",
        "    return embeddings.squeeze().numpy()"
      ],
      "metadata": {
        "id": "gE0g6v7ACvrX"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_documents(documents):\n",
        "    encoded_docs = []\n",
        "    for doc in documents:\n",
        "        inputs = tokenizer(doc, return_tensors='pt')\n",
        "        with torch.no_grad():\n",
        "            embeddings = model(**inputs).pooler_output\n",
        "        encoded_docs.append(embeddings.squeeze().numpy())\n",
        "    return np.array(encoded_docs)"
      ],
      "metadata": {
        "id": "xveAnzSzCxed"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_query(query):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(query):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return ' '.join(synonyms)"
      ],
      "metadata": {
        "id": "x6fICJ4cCzIS"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_bm25(query, documents):\n",
        "    if not documents:\n",
        "        return np.array([])  # Handle empty documents list\n",
        "    tokenized_docs = [doc.split() for doc in documents]\n",
        "    bm25 = BM25Okapi(tokenized_docs)\n",
        "    tokenized_query = query.split()\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "DPzooSxEC1Z9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_dpr(query, documents):\n",
        "    if not documents:\n",
        "        return np.array([])  # Handle empty documents list\n",
        "    query_embedding = encode_query(query)\n",
        "    document_embeddings = encode_documents(documents)\n",
        "    similarities = np.dot(document_embeddings, query_embedding)\n",
        "    return similarities\n"
      ],
      "metadata": {
        "id": "QCiDwyYYC3TR"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_documents(query, documents):\n",
        "    if not documents:\n",
        "        return []  # Handle empty documents list\n",
        "    bm25_scores = retrieve_bm25(query, documents)\n",
        "    dpr_scores = retrieve_dpr(query, documents)\n",
        "    if len(bm25_scores) == 0 or len(dpr_scores) == 0:\n",
        "        return []  # Handle empty scores\n",
        "    combined_scores = bm25_scores + dpr_scores\n",
        "    ranked_indices = np.argsort(combined_scores)[::-1]\n",
        "    return [(documents[i], combined_scores[i]) for i in ranked_indices]"
      ],
      "metadata": {
        "id": "3nIDvfb1C5tD"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4 & 5: Retrieval Augmented Generation (RAG) #"
      ],
      "metadata": {
        "id": "h8P3pOe0C8wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_from_context(context, query):\n",
        "    model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"   # Make sure to accept the terms on the model card.\n",
        "\n",
        "    pipeline = transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_id,\n",
        "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    # Prepare messages based on whether context is provided or not\n",
        "    if context:\n",
        "        prompt = f\"Context: {context}\\nQuery: {query}\\nResponse:\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant!\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    else:\n",
        "        prompt = f\"Query: {query}\\nResponse:\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant!\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "    # Generate response\n",
        "    outputs = pipeline(\n",
        "        messages,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "    # Extract and return generated text\n",
        "    output_text = outputs[0][\"generated_text\"][-1]['content']\n",
        "    return output_text\n"
      ],
      "metadata": {
        "id": "67zzixHXDAaC"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Main Function to Integrate All Tasks #"
      ],
      "metadata": {
        "id": "abNAWYUvDEac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(pdf_path, user_query):\n",
        "    # Task 1: Extract text from PDF\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Task 2: Create hierarchical index\n",
        "    hierarchical_index = create_hierarchical_index(extracted_text)\n",
        "    documents = [doc[\"content\"] for doc in hierarchical_index]\n",
        "\n",
        "    # Task 3: Retrieve relevant documents based on query\n",
        "    expanded_query = expand_query(user_query)\n",
        "    retrieved_docs_with_scores = retrieve_documents(expanded_query, documents)\n",
        "    relevant_texts = ' '.join([doc for doc, score in retrieved_docs_with_scores[:5]])  # Top 5 documents\n",
        "\n",
        "    # Task 4 & 5: Generate a response using RAG\n",
        "    response = generate_response_from_context(relevant_texts, user_query)\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "dx79_KEBDDAW"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mqgs1UQmMh1D"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = 'your_pdf_file_path'\n",
        "user_query = \"your_question_or_query\"\n",
        "response = main(pdf_path, user_query)\n",
        "print(f\"Response: {response}\")"
      ],
      "metadata": {
        "id": "dREal2lQDNeO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}